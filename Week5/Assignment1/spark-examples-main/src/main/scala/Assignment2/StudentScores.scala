package Assignment2


import org.apache.spark.sql.{SparkSession, functions => F}

object StudentScores {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder()
      .appName("Student Scores Sorting - 1.5M")
      .master("local[*]")
      .getOrCreate()

    import spark.implicits._

    import scala.util.Random

    spark.sparkContext.setLogLevel("ERROR")

    // --------------------------------------------------------------
    // üîπ DATA GENERATION (1.5M STUDENTS)
    // --------------------------------------------------------------
    val numStudents = 1500000

    val studentRDD = spark.sparkContext.parallelize(1 to numStudents, 20)
      .map { id =>
        val name = Random.alphanumeric.take(6).mkString
        val score = Random.nextInt(100)
        (id, name, score)
      }

    val studentDF = studentRDD.toDF("studentId", "name", "score")

    println("\n===== SAMPLE STUDENT DATA =====")
    studentDF.show(5, false)

    // --------------------------------------------------------------
    // üîπ SORT STUDENTS BY SCORE DESCENDING
    // --------------------------------------------------------------
    val sortedDF = studentDF.orderBy(F.col("score").desc)

    println("\n===== SORTED STUDENTS (Top 10) =====")
    sortedDF.show(10, false)

    // --------------------------------------------------------------
    // üîπ WRITE SORTED OUTPUT TO JSON
    // --------------------------------------------------------------
    val jsonPath = "output/student_scores_json"

    sortedDF.write
      .mode("overwrite")
      .json(jsonPath)

    println(s"\nJSON written to: $jsonPath")

    // --------------------------------------------------------------
    // üîπ OBSERVATIONS
    // --------------------------------------------------------------
    println(
      """
        |==================== OBSERVATIONS ====================
        |1Ô∏è‚É£ Sorting is a BROAD transformation.
        |   ‚Ä¢ Spark must SHUFFLE all 1.5M records across partitions.
        |   ‚Ä¢ Sorting requires global ordering ‚Üí triggers full shuffle.
        |
        |2Ô∏è‚É£ JSON writing is slow because:
        |   ‚Ä¢ JSON is row-based (cannot compress well)
        |   ‚Ä¢ Each row written as separate text object
        |   ‚Ä¢ CPU-heavy serialization
        |
        |3Ô∏è‚É£ Parquet/ORC would be MUCH faster due to:
        |   ‚Ä¢ Columnar format
        |   ‚Ä¢ Compression
        |   ‚Ä¢ Binary efficient encoding
        |
        |======================================================
        |""".stripMargin)

    Thread.sleep(300000)
    spark.stop()
  }
}
